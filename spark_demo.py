from pyspark.sql import SparkSession, functions as F
from pathlib import Path

# === SparkSession ===
spark = SparkSession.builder \
    .appName("MiPrimerJob-IAE") \
    .master("local[*]") \
    .getOrCreate()

root = Path(__file__).resolve().parent
csv_in = root / "data" / "ventas_100mb.csv"      # tu CSV real
out_parquet = root / "outputs" / "salida.parquet" # 3.7 (parquet "plano")
out_part = root / "outputs" / "salida_particionado"  # 3.7 particionado

# === 3.6 Leer un CSV y transformarlo ===
# OJO: tu columna se llama 'regiÃ³n' (con tilde). La renombro a 'region' para evitar problemas.
df = (spark.read
      .option("header", True)
      .option("inferSchema", True)
      .csv(str(csv_in)))

df = df.withColumnRenamed("regiÃ³n", "region")

# Ver columnas y primeras filas
print("ðŸ“„ Esquema:")
df.printSchema()
print("ðŸ‘€ Primeras filas:")
df.show(5, truncate=False)

# Filtro y selecciÃ³n
df_filtered = df.filter(F.col("monto") > 100).select("cliente", "monto")

# AgregaciÃ³n (promedio de monto por regiÃ³n)
df_grouped = df.groupBy("region").agg(F.avg("monto").alias("avg_monto"))

print("ðŸ“Š AgregaciÃ³n por regiÃ³n:")
df_grouped.show()

# === 3.8 Protip: visualizar el plan de ejecuciÃ³n ===
print("ðŸ§  Plan de df_filtered:")
df_filtered.explain()

# === 3.7 Guardar en Parquet ===
# Parquet "plano" (archivo/directorio Ãºnico)
df.write.mode("overwrite").parquet(str(out_parquet))

# Parquet particionado por regiÃ³n
df.write.mode("overwrite").partitionBy("region").parquet(str(out_part))

print(f"âœ… Parquet escrito en: {out_parquet}")
print(f"âœ… Parquet particionado en: {out_part}")

spark.stop()
