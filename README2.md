Aqu√≠ va tu **README 2** ‚Äîexplicaci√≥n detallada y pasos‚Äî listo para pegar como `README_PART2_SPARK.md` 

---

# Parte 2 ‚Äî De compresi√≥n a **Parquet particionado con Spark**

Este m√≥dulo extiende la etapa de **compresi√≥n** (CSV ‚Üí `.csv.gz`) y agrega una etapa de **ingenier√≠a de datos con PySpark**: conversi√≥n a **Parquet** y **particionado por `mes`** para mejorar performance de lectura y anal√≠tica.

## üéØ Objetivo

* Pasar de **un archivo comprimido** a un **dataset columnar optimizado**.
* Escribir los datos en **Parquet** (con compresi√≥n interna) y **particionar por `mes`** ‚Üí lectura selectiva por partici√≥n y mejor I/O.

---

## üß± Estructura actual del proyecto (relevante)

```
MODULO5COMPRESION/
‚îú‚îÄ .venv/
‚îú‚îÄ data/
‚îú‚îÄ outputs/
‚îÇ  ‚îú‚îÄ csv/
‚îÇ  ‚îÇ  ‚îú‚îÄ ventas_100mb.csv
‚îÇ  ‚îÇ  ‚îî‚îÄ ventas_100mb.csv.gz
‚îÇ  ‚îú‚îÄ parquet/                 (parquet "no particionado", opcional)
‚îÇ  ‚îú‚îÄ salida_particionado/     (parquet particionado por mes)  ‚úÖ
‚îÇ  ‚îÇ  ‚îú‚îÄ mes=2023-01/
‚îÇ  ‚îÇ  ‚îÇ  ‚îú‚îÄ 9077e3bd...-0.parquet
‚îÇ  ‚îÇ  ‚îÇ  ‚îî‚îÄ a10ff3ff...-0.parquet
‚îÇ  ‚îÇ  ‚îú‚îÄ mes=2023-02/
‚îÇ  ‚îÇ  ‚îî‚îÄ ...
‚îÇ  ‚îî‚îÄ _spark_test_write/       (carpeta de test creada por Spark, opcional)
‚îú‚îÄ .gitignore
‚îú‚îÄ benchmark_formatos.py
‚îú‚îÄ exportar_formatos.py
‚îú‚îÄ make_100mb_csv.py
‚îú‚îÄ make_partition.py           ‚úÖ script principal de esta parte
‚îú‚îÄ spark_demo.py
‚îú‚îÄ write_test.py
‚îî‚îÄ requirements.txt
```

---

## üß™ Qu√© hiciste en la **Parte 1 (Compresi√≥n)**

1. Generaste un CSV grande (`ventas_100mb.csv`).
2. Lo **comprimiste** a `ventas_100mb.csv.gz`.

   * Ventaja: ocupa menos espacio.
   * L√≠mite: **sigue siendo fila a fila**; no es formato columnar y no est√° particionado.

---

## ‚öôÔ∏è Qu√© hiciste ahora en la **Parte 2 (Spark)**

Con el script `make_partition.py`:

1. **Le√≠ste** el CSV fuente (desde `outputs/csv/ventas_100mb.csv` o el `.gz`).
2. **(Opcional)** Normalizaste tipos / columnas (por ejemplo, asegurar que `mes` est√© en formato `YYYY-MM`.

   * Si el CSV **ya trae** `mes`, Spark lo usa directo.
   * Si no, el script puede derivarlo desde una columna fecha (p. ej. `to_date` ‚Üí `date_format(..., 'yyyy-MM')`).
3. **Escribiste en Parquet** con compresi√≥n interna (columnar).
4. **Particionaste por `mes`** con `partitionBy("mes")`.
5. Resultado: `outputs/salida_particionado/mes=YYYY-MM/*.parquet`.

> **Nota de naming**: tus archivos aparecen como `9077e3bd....parquet` (hash/UUID). Es normal. En algunas configuraciones Spark usa `part-*.parquet`. **No es un error**.

---

## üß† ¬øPor qu√© Parquet + particiones?

* **Parquet**: formato columnar ‚Üí lecturas m√°s r√°pidas, compresi√≥n por columna, mejor para analytics.
* **Particiones por `mes`**: cuando filtr√°s por `mes`, el motor **lee solo esas carpetas** ‚Üí menos I/O, m√°s velocidad.
* **Compresi√≥n interna**: Parquet ya aplica compresi√≥n (snappy por defecto), sin perder la capacidad de consulta eficiente.

---

## ‚ñ∂Ô∏è C√≥mo reproducir (paso a paso)

### 0) Activar entorno e instalar dependencias

```powershell
# En PowerShell
.\.venv\Scripts\Activate
pip install -r requirements.txt
```

### 1) (Opcional) Generar el CSV de prueba

```powershell
python .\make_100mb_csv.py
```

### 2) Ejecutar el particionado con Spark

```powershell
python .\make_partition.py
```

### 3) Verificar la salida en disco

```powershell
# Ver carpetas de partici√≥n
Get-ChildItem .\outputs\salida_particionado

# Contar archivos por partici√≥n
Get-ChildItem .\outputs\salida_particionado -Recurse -File -Filter *.parquet |
  Group-Object { $_.Directory.Name } | Select-Object Name, Count
```

---

## üîé Validaciones r√°pidas

### A) Chequear versi√≥n de PySpark y Spark

> En PowerShell **no** escribas `import` directo. Us√° el REPL de Python o `-c`.

**REPL:**

```powershell
python
```

Dentro de Python:

```python
import pyspark
from pyspark.sql import SparkSession
print("PySpark:", pyspark.__version__)
print("Spark  :", SparkSession.builder.getOrCreate().version)
```

Salir: `exit()`.

**One-liner:**

```powershell
python -c "import pyspark; from pyspark.sql import SparkSession; \
print('PySpark:', pyspark.__version__); \
print('Spark  :', SparkSession.builder.getOrCreate().version)"
```

### B) Leer un Parquet (Pandas)

```powershell
python -c "import pandas as pd, glob; \
paths = glob.glob(r'.\outputs\salida_particionado\mes=2023-01\*.parquet'); \
df = pd.read_parquet(paths[0]); \
print(df.head()); print('Filas en primer archivo:', len(df))"
```

### C) Leer todo con Spark y contar

*(En PowerShell us√° REPL o `-c`; heredoc tipo `<<PY` es de bash.)*

```powershell
python -c "from pyspark.sql import SparkSession; \
spark = SparkSession.builder.getOrCreate(); \
df = spark.read.parquet(r'.\outputs\salida_particionado'); \
print('Total filas:', df.count()); \
df.show(5, truncate=False)"
```

---

## üìà Rendimiento (idea)

* **CSV (.gz)**: bueno para storage, **malo para an√°lisis** (escaneo total, sin pushdown de columnas).
* **Parquet particionado**: excelente en lectura filtrada y selecci√≥n de columnas; **menos CPU y disco**.

Pod√©s comparar con `benchmark_formatos.py` (si lo ten√©s implementado) midiendo tiempos de:

* Leer CSV vs Parquet.
* `count()` del total.
* Filtro por `mes`.

---

## üßØ Troubleshooting r√°pido

* **‚Äú`import` no reconocido‚Äù**: est√°s escribiendo Python en PowerShell. Entr√° al REPL (`python`) o us√° `python -c "..."`
* **No aparecen `part-*.parquet`**: no pasa nada; Spark puede nombrar con UUIDs. La estructura de **carpetas `mes=YYYY-MM`** es lo importante.
* **No se crea `salida_particionado`**: revis√° rutas de entrada/salida en `make_partition.py` y permisos de escritura.
* **Pandas no abre Parquet**: asegurate de tener `pyarrow` en `requirements.txt`.

---

## üß© Qu√© archivos de c√≥digo usaste ac√°

* `make_partition.py` ‚Üí **Script principal**: lee CSV/CSV.GZ, crea/ajusta `mes` si hace falta, y escribe Parquet **particionado**.
* `spark_demo.py` / `write_test.py` ‚Üí utilitarios opcionales de prueba de escritura Spark.
* `requirements.txt` ‚Üí incluye `pyspark`, `pandas`, `pyarrow`, etc.

---

## ‚úÖ Resumen ejecutivo

* **Antes:** CSV ‚Üí `.csv.gz` (**compresi√≥n lineal** de un √∫nico archivo).
* **Ahora:** CSV/CSV.GZ ‚Üí **Parquet particionado por `mes` con Spark** (columnar + compresi√≥n interna + lectura selectiva).
* **Resultado:** dataset anal√≠tico **mucho m√°s eficiente** para consultas y pipelines.

---


